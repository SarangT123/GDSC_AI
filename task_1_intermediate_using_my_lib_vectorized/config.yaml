network-architecture:
  - 784    # Input layer (28x28 MNIST images)
  - 256
  - 128
  - 64
  - 32
  - 10     # Output layer (digits 0â€“9)

network-functions:
  model-type: "1"
  # model 1 uses ReLU and Softmax(output) as activation functions and Cross-Entropy as loss function
  # model 2 uses Sigmoid as activation function and squared error as loss function

training-parameters:
  epochs: 20
  batch_size: 32
  learning_rate: 0.1
